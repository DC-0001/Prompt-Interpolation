{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate groq gradio\n"
      ],
      "metadata": {
        "id": "os41SvcFQNah"
      },
      "id": "os41SvcFQNah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1bb2ae-464e-449e-a45c-2a72586d0ff5",
      "metadata": {
        "id": "3f1bb2ae-464e-449e-a45c-2a72586d0ff5"
      },
      "outputs": [],
      "source": [
        "def generate(object1,object2):\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import time\n",
        "    from PIL import Image\n",
        "    from IPython import display as IPdisplay\n",
        "    from tqdm.auto import tqdm\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    from diffusers import (\n",
        "        DDIMScheduler,\n",
        "        PNDMScheduler,\n",
        "        LMSDiscreteScheduler,\n",
        "        DPMSolverMultistepScheduler,\n",
        "        EulerAncestralDiscreteScheduler,\n",
        "        EulerDiscreteScheduler,\n",
        "    )\n",
        "    from transformers import logging\n",
        "    logging.set_verbosity_error()\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    local_model_path = os.path.join(ROOT_DIR, \"sd15\")\n",
        "    save_path        = os.path.join(ROOT_DIR, \"outputs\")\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    scheduler = LMSDiscreteScheduler(\n",
        "            beta_start=0.00085,\n",
        "            beta_end=0.012,\n",
        "            beta_schedule=\"scaled_linear\",\n",
        "            num_train_timesteps=1000\n",
        "        )\n",
        "\n",
        "    if not os.path.exists(local_model_path):\n",
        "        print(\"Downloading and saving the model...\")\n",
        "\n",
        "\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            scheduler=scheduler,\n",
        "            torch_dtype=torch.float32,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.save_pretrained(local_model_path)\n",
        "        print(\"Model saved to:\", local_model_path)\n",
        "        del pipe\n",
        "    device=\"cuda\"\n",
        "\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    local_model_path,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float32,\n",
        "    safety_checker=None,\n",
        "    ).to(device)\n",
        "\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "    pipe.unet.to(memory_format=torch.channels_last)\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    def display(images, save_path):\n",
        "        try:\n",
        "            images = [\n",
        "                Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
        "            ]\n",
        "            filename = (\n",
        "                'stable_gif'\n",
        "\n",
        "            )\n",
        "            images[0].save(\n",
        "                f\"{save_path}/{filename}.gif\",\n",
        "                save_all=True,\n",
        "                append_images=images[1:],\n",
        "                duration=150,\n",
        "                loop=0,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        return IPdisplay.Image(f\"{save_path}/{filename}.gif\")\n",
        "\n",
        "    seed = None\n",
        "\n",
        "    if seed is not None:\n",
        "        generator = torch.manual_seed(seed)\n",
        "    else:\n",
        "        generator = None\n",
        "\n",
        "    guidance_scale = 10\n",
        "    num_inference_steps = 12\n",
        "    num_interpolation_steps = 17\n",
        "    height = 512\n",
        "    width = 512\n",
        "\n",
        "    save_path = \"./output\"\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    from groq import Groq\n",
        "\n",
        "    client = Groq(api_key=\"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\")\n",
        "\n",
        "    objects = [object1, object2]\n",
        "\n",
        "    # Base prompt template\n",
        "    base_prompt = (\n",
        "        \"Generate a background description for a {object}. \"\n",
        "        \"The description should be around 2-3 lines. \"\n",
        "    )\n",
        "\n",
        "    def get_description(obj):\n",
        "        prompt = base_prompt.format(object=obj)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",  #Llama-3.3 versatile model\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    # Base image generation\n",
        "    base_image_prompt = (\n",
        "    \"{description}\"\n",
        "    )\n",
        "\n",
        "    # Fixed negative prompt\n",
        "    negative_prompts = (\n",
        "    \"poorly drawn,cartoon, 2d, sketch, cartoon, drawing, anime, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\",\n",
        "    \"poorly drawn,cartoon, 2d, sketch, cartoon, drawing, anime, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\",\n",
        "    )\n",
        "\n",
        "    prompts = []\n",
        "    for obj in objects:\n",
        "        description = get_description(obj)\n",
        "        positive_prompt = base_image_prompt.format(object=obj, description=description)\n",
        "        prompts.append(positive_prompt)\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    # Tokenizing and encoding the prompts into embeddings\n",
        "    prompts_tokens = pipe.tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    prompts_embeds = pipe.text_encoder(\n",
        "        prompts_tokens.input_ids.to(device)\n",
        "    )[0]\n",
        "\n",
        "    if negative_prompts is None:\n",
        "        negative_prompts = [\"\"] * batch_size\n",
        "\n",
        "    negative_prompts_tokens = pipe.tokenizer(\n",
        "        negative_prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    negative_prompts_embeds = pipe.text_encoder(\n",
        "        negative_prompts_tokens.input_ids.to(device)\n",
        "    )[0]\n",
        "\n",
        "\n",
        "    def slerp(y0, y1, num, x0=0, x1=1):\n",
        "        y0 = y0.detach().cpu().numpy()\n",
        "        y1 = y1.detach().cpu().numpy()\n",
        "\n",
        "        def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "            dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "            if np.abs(dot) > DOT_THRESHOLD:\n",
        "                v2 = (1 - t) * v0 + t * v1\n",
        "            else:\n",
        "                theta_0 = np.arccos(dot)\n",
        "                sin_theta_0 = np.sin(theta_0)\n",
        "                theta_t = theta_0 * t\n",
        "                sin_theta_t = np.sin(theta_t)\n",
        "                s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "                s1 = sin_theta_t / sin_theta_0\n",
        "                v2 = s0 * v0 + s1 * v1\n",
        "            return v2\n",
        "\n",
        "        t = np.linspace(x0, x1, num)\n",
        "\n",
        "        y3 = torch.tensor(np.array([interpolation(t[i], y0, y1) for i in range(num)]))\n",
        "\n",
        "        return y3\n",
        "\n",
        "    latents = torch.randn(\n",
        "        (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "        generator=generator,\n",
        "    )\n",
        "\n",
        "    interpolated_prompt_embeds = []\n",
        "    interpolated_negative_prompts_embeds = []\n",
        "    for i in range(batch_size - 1):\n",
        "        interpolated_prompt_embeds.append(\n",
        "            slerp(\n",
        "                prompts_embeds[i],\n",
        "                prompts_embeds[i + 1],\n",
        "                num_interpolation_steps\n",
        "            )\n",
        "        )\n",
        "        interpolated_negative_prompts_embeds.append(\n",
        "            slerp(\n",
        "                negative_prompts_embeds[i],\n",
        "                negative_prompts_embeds[i + 1],\n",
        "                num_interpolation_steps,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    interpolated_prompt_embeds = torch.cat(\n",
        "        interpolated_prompt_embeds, dim=0\n",
        "    ).to(device)\n",
        "\n",
        "    interpolated_negative_prompts_embeds = torch.cat(\n",
        "        interpolated_negative_prompts_embeds, dim=0\n",
        "    ).to(device)\n",
        "\n",
        "    # Generating images using the embeddings\n",
        "    images = []\n",
        "    for prompt_embeds, negative_prompt_embeds in tqdm(\n",
        "        zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
        "        total=len(interpolated_prompt_embeds),\n",
        "    ):\n",
        "        images.append(\n",
        "            pipe(\n",
        "                height=height,\n",
        "                width=width,\n",
        "                num_images_per_prompt=1,\n",
        "                prompt_embeds=prompt_embeds[None, ...],\n",
        "                negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                generator=generator,\n",
        "                latents=latents,\n",
        "            ).images\n",
        "        )\n",
        "\n",
        "\n",
        "    gif_disp = display(images, save_path)\n",
        "    filename = \"stable_gif.gif\"\n",
        "    gif_path = os.path.join(save_path, filename)\n",
        "\n",
        "    return gif_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d7232a-59e0-42cf-853c-c271ef41e4a6",
      "metadata": {
        "id": "38d7232a-59e0-42cf-853c-c271ef41e4a6"
      },
      "outputs": [],
      "source": [
        "gif_path = run_generation(\"red ball\", \"blue ball\")\n",
        "gif_path\n",
        "from IPython.display import Image\n",
        "Image(filename=gif_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd872b50-d806-4153-b33a-3c1affe604cd",
      "metadata": {
        "id": "bd872b50-d806-4153-b33a-3c1affe604cd"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def interface(obj1, obj2):\n",
        "    gif_path = generate(obj1, obj2)\n",
        "    return gif_path\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Text-to-Image Interpolation\")\n",
        "    gr.Markdown(\"Enter two objects below and generate an interpolated image GIF using Stable Diffusion.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            obj1 = gr.Textbox(label=\"Object 1\", placeholder=\"e.g., dog\")\n",
        "        with gr.Column(scale=1):\n",
        "            obj2 = gr.Textbox(label=\"Object 2\", placeholder=\"e.g., cat\")\n",
        "\n",
        "    gen_btn = gr.Button(\"ðŸš€ Generate\")\n",
        "\n",
        "    output_img = gr.Image(\n",
        "        type=\"filepath\",\n",
        "        format=\"gif\",\n",
        "        label=\"ðŸ”„ Interpolated GIF\",\n",
        "        show_label=True\n",
        "    )\n",
        "    gen_btn.click(fn=interface, inputs=[obj1, obj2], outputs=output_img)\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "demo.launch(allowed_paths=[\"./output\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}